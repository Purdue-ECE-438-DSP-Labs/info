{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>\n",
    "    ECE 438 - Laboratory 10b<br/>\n",
    "    Image Processing (Week 2)<br/>\n",
    "    <small>Last updated on April 24, 2022</small>\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the plot is displayed in this notebook\n",
    "%matplotlib inline\n",
    "# specify the size of the plot\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon;\"><left>1. Introduction</left></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second part of a two week experiment in image processing. In the first week , we covered the fundamentals of digital monochrome images, intensity histograms, pointwise transformations, gamma correction, and image enhancement based on filtering. \n",
    "\n",
    "During this week, we will cover some fundamental concepts of color images. This will include a brief description on how humans perceive color, followed by descriptions of two standard **color spaces**. We will also discuss an application known as **halftoning**, which is the process of converting a gray scale image into a binary image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon;\"><left>2. Color Images</left></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:salmon;\"><left>2.1. Background on Color</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color is a perceptual phenomenon related to the human response to different wavelengths of light, mainly in the region of $400$ to $700$ nanometers (nm). The perception of color arises from the sensitivities of three types of neurochemical sensors in the retina, known as the long ($L$), medium ($M$), and short ($S$) cones. The response of these sensors to photons is shown in Figure 1. Note that each sensor responds to a range of wavelengths.\n",
    "\n",
    "Due to this property of the human visual system, all colors can be modeled as combinations of the three primary color components: red ($R$), green ($G$), and blue ($B$). For the purpose of standardization, the CIE (Commission International de l’Eclairage — the International Commission on Illumination) designated the following wavelength values for the three primary colors: $\\text{blue} = 435.8\\text{ nm}$, $\\text{green} = 546.1\\text{ nm}$, and $\\text{red} = 700\\text{ nm}$.\n",
    "\n",
    "The relative amounts of the three primary colors of light required to produce a color of a given wavelength are called *tristimulus values*. Figure 2 shows the plot of tristimulus values using the CIE primary colors. Notice that some of the tristimulus values are negative, which indicates that colors at those wavelengths cannot be reproduced by the CIE primary colors.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure1.png\" style=\"width:40%\">\n",
    "    <em><center>Figure 1: Relative photon sensitivity of long ($L$), medium ($M$), and short ($S$) cones.</center></em>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure2.png\" style=\"width:40%\">\n",
    "    <em><center>Figure 2: Plot of tristimulus values using CIE primary colors.</center></em>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:salmon;\"><left>2.2. Color Spaces</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A color space allows us to represent all the colors perceived by human beings. We previously noted that weighted combinations of stimuli at three wavelengths are sufficient to describe all the colors we perceive. These wavelengths form a natural basis, or coordinate system, from which the color measurement process can be described. In this lab, we will examine two common color spaces: $RGB$ and $YC_bC_r$. For more information, refer to [1].\n",
    "\n",
    "* $RGB$ space is one of the most popular color spaces, and is based on the tristimulus theory of human vision, as described above. The RGB space is a hardware-oriented model, and is thus primarily used in computer monitors and other raster devices. Based upon this color space, each pixel of a digital color image has three components: red, green, and blue.\n",
    "* $YC_bC_r$ space is another important color space model. This is a gamma corrected space defined by the CCIR (International Radio Consultative Committee), and is mainly used in the digital video paradigm. This space consists of luminance ($Y$) and chrominance ($C_bC_r$) components. The importance of the $YC_bC_r$ space comes from the fact that the human visual system perceives a color stimulus in terms of luminance and chrominance attributes, rather than in terms of $R$, $G$, and $B$ values. The relation between $YC_bC_r$ space and gamma corrected $RGB$ space is given by the following linear transformation:\n",
    "\n",
    "\\begin{align*}\n",
    "    Y&=0.299R+0.587G+0.114B\\\\\n",
    "    C_b&=0.564(B-Y)+128\\tag{1}\\\\\n",
    "    C_r&=0.713(R-Y)+128\n",
    "\\end{align*}\n",
    "\n",
    "In $YC_bC_r$, the luminance parameter is related to an overall intensity of the image. The chrominance components are a measure of the relative intensities of the blue and red components. The inverse of the transformation in equation (1) can easily be shown to be the following:\n",
    "\n",
    "\\begin{align*}\n",
    "    R&=Y+1.4025(C_r-128)\\\\\n",
    "    G&=Y-0.3443(C_b-128)-0.7144(C_r-128)\\tag{2}\\\\\n",
    "    B&=Y+1.7730(C_b-128)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"><left>Exercise 2.3: Color</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the image file ```girl.tif```. Check the size of array for this image by using the command ```print(image.shape)```, where ```image``` is the image matrix. Also, print the data type of this matrix.** \n",
    "\n",
    "Notice that this is a three dimensional array of type ```uint8```. It contains three gray scale image planes corresponding to the red, green, and blue components for each pixel. Since each color pixel is represented by three bytes, this is commonly known as a 24-bit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Display the image. Note that ```cmap```, ```vmin```, ```vmax``` arguments are not needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Extract each of the color components, then plot each color component.**\n",
    "\n",
    "Note that while the original is a color image, each color component separately is a monochrome image, so plotting each color component requires ```cmap```, ```vmin```, ```vmax``` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Load the files ```ycbcr.npy``` using [`np.load()`](https://numpy.org/doc/stable/reference/generated/numpy.load.html), and print its type and data shape `dtype`.**\n",
    "\n",
    "This file contains a NumPy array for a color image in $YC_bC_r$ format. The array contains three gray scale image planes that correspond to the luminance ($Y$) and two chrominance ($C_bC_r$) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Plot each of the components.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly display this color image, we need to convert it to $RGB$ format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Complete the function below that will perform the transformation of equation (2). It should accept a 3-D $YC_bC_r$ image array as input, and return a 3-D $RGB$ image array.**\n",
    "\n",
    "* Make sure `ycbcr` is in `double` or `float` before any processing.\n",
    "* After conversion, to make sure the values of `rgb` are in $[0,255]$, use [`np.clip()`](https://numpy.org/doc/stable/reference/generated/numpy.clip.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ycbcr2rgb(ycbcr):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    ycbcr: image in YCbCr\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    rgb: image RGB\n",
    "    \"\"\"\n",
    "    \n",
    "    rgb = None\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Now, convert the ycbcr array to an RGB representation and display the color image.**\n",
    "\n",
    "* Before displaying the image, make sure its data type is `np.uint8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property of the human visual system, with respect to the $YC_bC_r$ color space, is that we are much more sensitive to distortion in the luminance component than in the chrominance components. To illustrate this, we will smooth each of these components with a Gaussian filter and view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load the file ```h.npy```. This is a $5 \\times 5$ Gaussian filter with $\\sigma^2=2.0$. (See the first week of the experiment for more details on this type of filter.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Alter the ```ycbcr``` array by filtering only the luminance component, ```ycbcr[:,:,0]```, using the Gaussian filter (use `convolve2d()` function from last lab). Convert the result to RGB, and display it.**\n",
    "\n",
    "* Instead of altering the original `ycbcr`, you can create a copy by `ycbcr1 = ycbcr.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    image: the input image\n",
    "    kernel: the filter\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    filtered: the filtered image\n",
    "    \"\"\"\n",
    "    filtered = None\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Now alter `ycbcr` by filtering both chrominance components, ```ycbcr[:,:,1]``` and ```ycbcr[:,:,2]```, using the Gaussian filter. Convert this result to RGB, and display it.**\n",
    "\n",
    "* Again, instead of altering the original `ycbcr`, you can create a copy by `ycbcr2 = ycbcr.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see a significant difference between the filtered versions and the original image? This is the reason that $YC_bC_r$ is often used for digital video. Since we are not very sensitive to corruption of the chrominance components, we can afford to lose some information in the encoding process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon;\"><left>3. Halftoning</left></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will cover a useful image processing technique called halftoning. The process of halftoning is required in many present day electronic applications such as facsimile (FAX), electronic scanning/copying, laser printing, and low bandwidth remote sensing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:salmon;\"><left>3.1. Binary Images</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was discussed in the first week of this lab, an $8$-bit monochrome image allows $256$ distinct gray levels. Such images can be displayed on a computer monitor if the hardware supports the required number intensity levels. However, some output devices print or display images with much fewer gray levels. In the extreme case, the gray scale images must be converted to binary images, where pixels can only be black or white.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure3.png\" style=\"width:100%\">\n",
    "    <em><center>Figure 3:  (a) Original gray scale image. (b) Binary image produced by simple fixed thresholding.</center></em>\n",
    "</figure>\n",
    "\n",
    "The simplest way of converting to a binary image is based on thresholding, i.e. two-level (one-bit) quantization. Let $f[i, j]$ be a gray scale image, and $b[i, j]$ be the corresponding binary image based on thresholding. For a given threshold $T$, the binary image is computed as the following:\n",
    "\n",
    "$$b[i,j]=\\begin{cases}255\\quad&\\text{if }f[i,j]>T\\\\0\\quad&\\text{else}\\end{cases}\\tag{3}$$\n",
    "\n",
    "Figure 3 shows an example of conversion to a binary image via thresholding, using $T = 80$.\n",
    "\n",
    "It can be seen in Figure 3 that the binary image is not “shaded” properly–an artifact known as false contouring. False contouring occurs when quantizing at low bit rates (one bit in this case) because the quantization error is dependent upon the input signal. If one reduces this dependence, the visual quality of the binary image is usually enhanced.\n",
    "\n",
    "One method of reducing the signal dependence of the quantization error is to add uniformly distributed white noise to the input image prior to quantization. To each pixel of the gray scale image $f[i, j]$, a white random number n in the range $[−A, A]$ is added, and then the resulting image is quantized by a one-bit quantizer, as in equation (3). The result of this method is illustrated in Figure 4, where the additive noise is uniform over $[−40, 40]$.\n",
    "Notice that even though the resulting binary image is somewhat noisy, the false contouring has been significantly reduced.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure4.png\" style=\"width:60%\">\n",
    "    <em><center>Figure 4: Random noise binarization.</center></em>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"><left>Exercise 3.2: Halftoning - Simple Thresholding</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the grayscale image file ```house.tif``` and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Try the simple thresholding technique based on equation (3), using $T = 108$, and display the result.**\n",
    "\n",
    "* In Python, an easy way to threshold an image $X$ is to use the command ```Y = 255 * (X > T)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create an “absolute error” image by subtracting the binary from the original image, and then taking the absolute value. The degree to which the original image is present in the error image is a measure of signal dependence of the quantization error. Display the error image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compute the mean square error (MSE), which is defined by**\n",
    "\n",
    "$$\\text{MSE}=\\frac{1}{MN}\\sum_{i,j}\\left\\{f[i,j]-b[i,j]\\right\\}^2\\tag{9}$$\n",
    "\n",
    "**where $MN$ is the total number of pixels in each image, $f$ is the original image and $b$ is the binarized image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:salmon;\"><left>3.3. Ordered Dithering</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halftone images are binary images that appear to have a gray scale rendition. Although the random thresholding technique described in Section 3.1 can be used to produce a halftone image, it is not often used in real applications since it yields very noisy results. In this section, we will describe a better halftoning technique known as ordered dithering.\n",
    "\n",
    "The human visual system tends to average a region around a pixel instead of treating each pixel individually, thus it is possible to create the illusion of many gray levels in a binary image, even though there are actually only two gray levels. With $2 \\times 2$ binary pixel grids, we can represent $5$ different “effective” intensity levels, as shown in Figure 5. Similarly for $3 \\times 3$ grids, we can represent $10$ distinct gray levels. In dithering, we replace blocks of the original image with these types of binary grid patterns.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure5.png\" style=\"width:80%\">\n",
    "    <em><center>Figure 5: Five different patterns of $2 \\times 2$ binary pixel grids.</center></em>\n",
    "</figure>\n",
    "\n",
    "Remember from Section 3.1 that false contouring artifacts can be reduced if we can reduce the signal dependence of the quantization error. We showed that adding uniform noise to the monochrome image can be used to achieve this decorrelation. An alternative method would be to use a variable threshold value for the quantization process.\n",
    "\n",
    "Ordered dithering consists of comparing blocks of the original image to a 2-D grid, known as a dither pattern. Each element of the block is then quantized using the corresponding value in the dither pattern as a threshold. The values in the dither matrix are fixed, but are typically different from each other. Because the threshold value varies between adjacent pixels, some decorrelation from the quantization error is achieved, which has the effect of\n",
    "reducing false contouring.\n",
    "\n",
    "The following is an example of a $2 \\times 2$ dither matrix,\n",
    "\n",
    "$$T[i,j]=255\\times\\begin{bmatrix}5/8&3/8\\\\1/8&7/8\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "This is a part of a general class of optimum dither patterns known as Bayer matrices. The values of the threshold matrix $T[i, j]$ are determined by the order that pixels turn “ON”. The order can be put in the form of an index matrix. For a Bayer matrix of size $2$, the index matrix $I[i, j]$ is given by\n",
    "\n",
    "$$I[i,j]=\\begin{bmatrix}3&2\\\\1&4\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "and the relation between $T[i, j]$ and $I[i, j]$ is given by\n",
    "\n",
    "$$T[i,j]=\\frac{255\\times(I[i,j]-0.5)}{n^2}\\tag{6}$$\n",
    "\n",
    "where $n^2$ is the total number of elements in the matrix.\n",
    "\n",
    "Figure 6 shows the halftone image produced by Bayer dithering of size $4$. It is clear from the figure that the halftone image provides good detail rendition. However the inherent square grid patterns are visible in the halftone image.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure6.png\" style=\"width:60%\">\n",
    "    <em><center>Figure 6: The halftone image produced by Bayer dithering of size $4$.</center></em>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"><left>Exercise 3.4: Halftoning - Ordered Dithering</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try implementing Bayer dithering of size $4$. You will first have to compute the dither pattern. The index matrix for a dither pattern of size $4$ is given by\n",
    "\n",
    "$$I[i,j]=\\begin{bmatrix}12&8&10&6\\\\4&16&2&14\\\\9&5&11&7\\\\1&13&3&15\\end{bmatrix}\\tag{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Based on this index matrix and equation (6), create the corresponding threshold matrix and print it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ordered dithering, it is easiest to perform the thresholding of the image all at once. This can be done by creating a large threshold matrix by repeating the $4 \\times 4$ dither pattern. A useful command here is ```np.tile(m, (num_y, num_x))```, where ```m``` is the matrix to be repeated, ```num_y``` is the repeated times in the vertical direction, and ```num_x``` is the repeated times in the horizontal direction.\n",
    "\n",
    "The thresholding can then be performed using the command ```Y = 255 * (X > m)``` where `X` and `Y` are the input and the output images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Apply the ordered dithering and display the halftoned image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compute the error image and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compute the MSE of the error image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:salmon;\"><left>3.5. Error Diffusion</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for halftoning is random dithering by error diffusion. In this case, the pixels are quantized in a specific order (raster ordering<sup>$\\dagger$</sup> is commonly used), and the residual quantization error for the current pixel is propagated (diffused) forward to unquantized pixels. This keeps the overall intensity of the output binary image closer to the input gray scale intensity.\n",
    "\n",
    "$\\dagger$: Raster ordering of an image orients the pixels from left to right, and then top to bottom. This is similar to the order that a CRT scans the electron beam across the screen.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure7.png\" style=\"width:60%\">\n",
    "    <em><center>Figure 7: Block diagram of the error diffusion method.</center></em>\n",
    "</figure>\n",
    "\n",
    "Figure 7 is a block diagram that illustrates the method of error diffusion. The current input pixel $f[i, j]$ is modified by means of past quantization errors to give a modified input $\\tilde{f}[i, j]$. This pixel is then quantized to a binary value by $Q$, using some threshold $T$. The error $e[i, j]$ is defined as\n",
    "\n",
    "$$e[i,j]=\\tilde{f}[i,j]-b[i,j]\\tag{7}$$\n",
    "\n",
    "where $b[i, j]$ is the quantized binary image.\n",
    "\n",
    "The error $e[i, j]$ of quantizing the current pixel is is diffused to “future” pixels by means of a two-dimensional weighting filter $h[i, j]$, known as the diffusion filter. The process of modifying an input pixel by past errors can be represented by the following recursive relationship.\n",
    "\n",
    "$$\\tilde{f}[i,j]=f[i,j]+\\sum_{k,l\\in S}h[k,l]e[i-k,j-l]\\tag{8}$$\n",
    "\n",
    "The most popular error diffusion method, proposed by Floyd and Steinberg, uses the diffusion filter shown in Figure 8. Since the filter coefficients sum to one, the local average value of the quantized image is equal to the local average gray scale value. Figure 9 shows the halftone image produced by Floyd and Steinberg error diffusion. Compared to the ordered dither halftoning, the error diffusion method can be seen to have better contrast performance. However, it can be seen in Figure 9 that error diffusion tends to create “streaking” artifacts,\n",
    "known as worm patterns.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure8.png\" style=\"width:20%\">\n",
    "    <em><center>Figure 8: The error diffusion filter proposed by Floyd and Steinberg.</center></em>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/figure9.png\" style=\"width:60%\">\n",
    "    <em><center>Figure 9: A halftone image produced by the Floyd and Steinberg error diffusion method.</center></em>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"><left>Exercise 3.6: Halftoning - Error Diffusion</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try halftoning via the error diffusion technique, using a threshold $T = 108$ and the diffusion filter in Figure 8. It is most straightforward to implement this by performing the following steps on each pixel in raster order:\n",
    "\n",
    "1. Initialize an output image matrix with zeros.\n",
    "2. Quantize the current pixel using using the threshold $T$, and place the result in the output matrix.\n",
    "3. Compute the quantization error by subtracting the binary pixel from the gray scale pixel.\n",
    "4. Add scaled versions of this error to “future” pixels of the original image, as depicted by the diffusion filter of Figure 8.\n",
    "5. Move on to the next pixel.\n",
    "\n",
    "You do not have to quantize the outer border of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Use the algorithm to create the halftoned image and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Compute the error image and display it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compute the MSE of the error image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. By comparing three MSE values, is the MSE consistent with the visual quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. By looking at the error images, determine which method appears to be the least signal dependent? Does the signal dependence seem to be correlated with the visual quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\"><left>Exercise 3.7: Halftoning - Filtered Halftone</left></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The human visual system naturally lowpass filters halftone images. To analyze this phenomenon, filter each of the halftone images with the Gaussian lowpass filter `h` that you loaded from `h.npy`, and measure the MSE of the filtered versions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use the following template to make a table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Halftone Method     |   Filtered    | Not filtered | \n",
    "|:-------------------:|:-------------:|:------------:|   \n",
    "| Simple Thresholding |               |              |   \n",
    "| Ordered Dithering   |               |              |  \n",
    "| Error Diffusion     |               |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compare the MSE’s of the filtered versions with the nonfiltered versions for each method. What is the implication of these observations with respect to how we perceive halftone images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:salmon;\"><left>4. References</left></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] J.M. Kasson and W. Plouffe, “An analysis of selected computer interchange color spaces,” ACM Trans. Graphics, vol. 11, no. 4, pp. 373–405, 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
